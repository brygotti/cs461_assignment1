{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"background-color:#FFFFFF\">   \n",
    "  <tr>     \n",
    "  <td><img src=\"https://upload.wikimedia.org/wikipedia/commons/9/95/Logo_EPFL_2019.svg\" width=\"150x\"/>\n",
    "  </td>     \n",
    "  <td>\n",
    "  <h1> <b>CS-461: Foundation Models and Generative AI</b> </h1>\n",
    "  Prof. Charlotte Bunne  \n",
    "  </td>   \n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìö Graded Assignment 1  \n",
    "### CS-461: Foundation Models and Generative AI - Fall 2025  - Due: October 8, 23:59 CET\n",
    "\n",
    "Welcome to the first graded assignment!\n",
    "In this assignment, you will **implement and explore self-supervised learning** on a downsampled subset of the [ImageNet-1k dataset](https://www.image-net.org/), and evaluate how well your model generalizes **both in-distribution and out-of-distribution (OOD)**.  \n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Learning Objectives\n",
    "By completing this assignment, you will learn to:\n",
    "- Implement a custom **encoder** and **projection head** for images  \n",
    "- Experiment with **data augmentations** for self-supervised learning  \n",
    "- Train a model using a **self-supervised loss**  \n",
    "- Evaluate learned representations with **k-NN** and **linear probes**  \n",
    "- Assess **out-of-distribution (OOD) generalization** to unseen classes  \n",
    "- Save, visualize, and submit results in a reproducible way  \n",
    "\n",
    "---\n",
    "\n",
    "## ‚ö° Practical Notes\n",
    "- **Dataset:**  \n",
    "  - Training: 200 ImageNet classes, 500 images each (100k total)  \n",
    "  - Validation: 200 ImageNet classes, 50 images each (10k total)  \n",
    "  - **OOD dataset:** 200 unseen classes, 50 images each (10k total)  \n",
    "- Use OOD only for **evaluation**, never for training.  \n",
    "- Checkpoints and evaluation intervals are already set up ‚Äî your main tasks are to fill in missing functions and customize the model.  \n",
    "- Some helper utilities (e.g., dataset loaders, probes) are provided in `utils.py`.  \n",
    "\n",
    "---\n",
    "\n",
    "üëâ **Deliverables:** You will submit:\n",
    "- Your modified **`models.py`**  \n",
    "- Trained weights in **`final_model.safetensors`**  \n",
    "- A short **report.md** (max 500 words) ‚Äî including **discussion of OOD results**  \n",
    "- This completed notebook **CS461_Assignment1.ipynb**  \n",
    "\n",
    "---\n",
    "\n",
    "‚ö†Ô∏è **Important:** Don‚Äôt forget to fill in your **SCIPER number** and **full name** in Section 0, otherwise you will receive **0 points**.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we import packages and set up the device. \\\n",
    "Feel free to add any additional packages you may need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Automatically reloads modules when you make changes (useful during development)\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import shutil\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.axes_grid1 import ImageGrid\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "from torch.utils.data import DataLoader\n",
    "from safetensors.torch import save_model\n",
    "\n",
    "import wandb\n",
    "from torchsummary import summary\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üÜî 0. SCIPER Number and Name  \n",
    "\n",
    "‚ö†Ô∏è **IMPORTANT!** ‚ö†Ô∏è  \n",
    "You **must** fill in your **SCIPER number** and **full name** below.  \n",
    "\n",
    "This is **required for automatic grading**.  \n",
    "If you do **not** provide this information, you will receive **0Ô∏è‚É£ (zero)** for this assignment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SCIPER = 325403  # Replace with your SCIPER number\n",
    "LAST_NAME = \"Gotti\"  # Replace with your last name\n",
    "FIRST_NAME = \"Bryan\"  # Replace with your first name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Datasets & Utilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In the following, we will work with a subset of the ImageNet-1k dataset: color images downsampled to 64√ó64, covering 200 classes.\n",
    "- The training set contains 500 images per class (100,000 images in total), and the validation set contains 50 images per class (10,000 images in total).\n",
    "- The Out-Of-Distribution (OOD) datasets contain images from classes not present in the training set. It contains 50 images from 200 different classes (1,000 images in total).\n",
    "- The purpose of these OOD datasets is to evaluate the generalization capabilities of the learned representations. You should not use it for training.\n",
    "- During evalution, we will measure your model's performance on another OOD dataset (different from the one provided here), so make sure to not overfit on the provided OOD dataset.\n",
    "\n",
    "<!-- Let's download/load it and define a default transformation turning a PIL Image into a `torch.tensor` -->\n",
    "Make sure that you have access to the `/shared/CS461/cs461_assignment1_data/` folder. The folder structure should look like this:\n",
    "```\n",
    "cs461_assignment1_data/\n",
    "‚îî‚îÄ‚îÄ train.npz\n",
    "‚îî‚îÄ‚îÄ val.npz\n",
    "‚îî‚îÄ‚îÄ ood.npz\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import dataset class and other utilities you developed in previous homeworks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import ImageDatasetNPZ, default_transform, seed_all\n",
    "from utils import run_knn_probe, run_linear_probe, extract_features_and_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For reproducibility, you can use the provided `seed_all` function to set the random seed for all relevant libraries (Python, NumPy, PyTorch)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_all(42)  # For reproducibility, you can use any integer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You probably want to implement custom data augmentations for the self-supervised learning method you choose. \\\n",
    "Feel free to swap the `default_transform` defined below and create multiple instances of datasets with different transforms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimCLRTransform:\n",
    "\n",
    "    def __init__(self, size=32, s=0.5, blur_p=0.5):\n",
    "        color_jitter = T.ColorJitter(0.8*s, 0.8*s, 0.8*s, 0.2*s)\n",
    "        k = 3 if size <= 32 else 5\n",
    "        base = [\n",
    "            T.ToPILImage(),\n",
    "            T.RandomResizedCrop(size=size, scale=(0.2, 1.0)),\n",
    "            T.RandomHorizontalFlip(),\n",
    "            T.RandomApply([color_jitter], p=0.8),\n",
    "            T.RandomGrayscale(p=0.2),\n",
    "            T.RandomApply([T.GaussianBlur(kernel_size=k, sigma=(0.1, 2.0))], p=blur_p),\n",
    "            T.ToTensor()\n",
    "        ]\n",
    "        self.train_transform = T.Compose(base)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        return self.train_transform(x), self.train_transform(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = Path('./cs461_assignment1_data/')\n",
    "checkpoints_dir = Path('checkpoints')\n",
    "if not checkpoints_dir.exists():\n",
    "    checkpoints_dir.mkdir(parents=True, exist_ok=False)\n",
    "final_model_path = checkpoints_dir / 'model_final.safetensors'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simclr_transform = SimCLRTransform(size=64)\n",
    "\n",
    "train_datasets = [ImageDatasetNPZ(data_dir / 'train.npz', transform=simclr_transform) for i in range(4)]\n",
    "train_dataset_no_augment = ImageDatasetNPZ(data_dir / 'train.npz', transform=default_transform)\n",
    "val_dataset_no_augment = ImageDatasetNPZ(data_dir / 'val.npz', transform=default_transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can split the provided OOD dataset into a training and validation set using the code below. \\\n",
    "You should not use the training split for actually training your models, but only for evaluation (e.g. kNN or linear probing)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.RandomState(42)\n",
    "ds_ood = ImageDatasetNPZ(data_dir / 'ood.npz', transform=default_transform)\n",
    "ood_val_ratio = 0.2\n",
    "train_mask = rng.permutation(len(ds_ood)) >= int(len(ds_ood) * ood_val_ratio)\n",
    "ds_oods_train = torch.utils.data.Subset(ds_ood, np.where(train_mask)[0])\n",
    "ds_oods_val = torch.utils.data.Subset(ds_ood, np.where(~train_mask)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "num_workers = 4\n",
    "pin_memory = True\n",
    "\n",
    "def collate_fn(batch):\n",
    "    xs1, xs2, ys = [], [], []\n",
    "    for (x1, x2), y in batch:\n",
    "        xs1.append(x1)\n",
    "        xs2.append(x2)\n",
    "        ys.append(y)\n",
    "    return torch.stack(xs1), torch.stack(xs2), torch.tensor(ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loaders = [DataLoader(td, batch_size=batch_size, num_workers=num_workers, pin_memory=pin_memory, shuffle=True, collate_fn=collate_fn) for td in train_datasets]\n",
    "train_loader_no_augment = DataLoader(train_dataset_no_augment, batch_size=batch_size, num_workers=num_workers, pin_memory=pin_memory, shuffle=True)\n",
    "val_loader_no_augment  = DataLoader(val_dataset_no_augment,  batch_size=batch_size, num_workers=num_workers, pin_memory=pin_memory, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oods_train_loader = DataLoader(ds_oods_train, batch_size=batch_size, num_workers=num_workers, pin_memory=pin_memory, shuffle=True)\n",
    "oods_val_loader = DataLoader(ds_oods_val, batch_size=batch_size, num_workers=num_workers, pin_memory=pin_memory, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Load Your Model\n",
    "\n",
    "- Load your model from `models.py`.\n",
    "- You will need to modify the `encoder` and `projection` modules, as the provided template implementation is only a placeholder.\n",
    "- You SHOULD NOT change the `input_dim`, `input_channels`, and `feature_dim` parameters of the `ImageEncoder` class.\n",
    "- You can use an existing architecture (e.g., ResNet, ViT) but you SHOULD NOT use any pre-trained weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import ImageEncoder\n",
    "\n",
    "\n",
    "model = ImageEncoder().to(device)\n",
    "summary(model, input_size=(3, 64, 64))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Helpers for Training & Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We suggest you to implement the following helper functions to keep your training and evaluation loops clean and organized. \\\n",
    "- `training_step`: Performs a single training step (forward pass, loss computation, backward pass, optimizer step) and returns the loss value.\n",
    "- `evaluation_step`: Evaluates the model on the validation dataset and returns the accuracy.\n",
    "\n",
    "Depending on your specific requirements, you may also want to implement additional utility functions for tasks such as data loading, metric computation, and logging.\n",
    "\n",
    "As you have seen from previous assignments, loss functions for self-supervised learning objectives can be quite complex. \\\n",
    "Feel free to implement any helper functions you may need to compute the loss.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_step(*args, **kwargs):\n",
    "    model, train_loaders, optimizer, custom_loss_function = args\n",
    "    avg_loss = 0.\n",
    "    for tl in train_loaders:\n",
    "        for x1, x2, y in tqdm(tl):\n",
    "            x1, x2 = x1.to(device), x2.to(device)\n",
    "\n",
    "            with torch.autocast(device_type='cuda', dtype=torch.bfloat16):\n",
    "                proj1 = model(x1)\n",
    "                proj2 = model(x2)\n",
    "            loss = custom_loss_function(proj1, proj2)\n",
    "    \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "            avg_loss += loss.item()\n",
    "    return { \"loss\": avg_loss / sum([len(tl) for tl in train_loaders]) }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation_step(*args, **kwargs):\n",
    "    model, train_loader_no_augment, val_loader_no_augment, oods_train_loader, oods_val_loader = args\n",
    "    \n",
    "    train_features, train_labels = extract_features_and_labels(model, train_loader_no_augment)\n",
    "    test_features, test_labels = extract_features_and_labels(model, val_loader_no_augment)\n",
    "    oods_train_features, oods_train_labels = extract_features_and_labels(model, oods_train_loader)\n",
    "    oods_test_features, oods_test_labels = extract_features_and_labels(model, oods_val_loader)\n",
    "    \n",
    "    knn_accuracy = run_knn_probe(train_features, train_labels, test_features, test_labels)\n",
    "    linear_accuracy = run_linear_probe(train_features, train_labels, test_features, test_labels)\n",
    "    oods_knn_accuracy = run_knn_probe(oods_train_features, oods_train_labels, oods_test_features, oods_test_labels)\n",
    "    oods_linear_accuracy = run_linear_probe(oods_train_features, oods_train_labels, oods_test_features, oods_test_labels)\n",
    "\n",
    "    return { \"knn_accuracy\": knn_accuracy, \"linear_accuracy\": linear_accuracy, \"oods_knn_accuracy\": oods_knn_accuracy, \"oods_linear_accuracy\": oods_linear_accuracy }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_loss_function(z1, z2, tau=0.5):\n",
    "    \"\"\"\n",
    "    Computes NT-Xent loss.\n",
    "    z1: (batch_size, feature_dim) tensor of normalized projection vectors\n",
    "    z2: (batch_size, feature_dim) tensor of normalized projection vectors\n",
    "    returns: loss (scalar)\n",
    "    \"\"\"\n",
    "    B, d = z1.shape\n",
    "    z = torch.cat([z1, z2], dim=0)              # (2B, d)\n",
    "    sim = (z @ z.t()) / tau                     # (2B, 2B)\n",
    "    mask = torch.eye(2*B, dtype=torch.bool, device=z.device)\n",
    "    sim.masked_fill_(mask, -1e9)\n",
    "    targets = torch.arange(B, device=z.device)\n",
    "    targets = torch.cat([targets + B, targets], dim=0)\n",
    "    return F.cross_entropy(sim, targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Optimizer Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_epochs = 50  # Adjust the number of epochs as needed\n",
    "warmup_epochs = 10\n",
    "\n",
    "def lr_lambda(epoch):\n",
    "    if epoch < warmup_epochs:\n",
    "        return (epoch + 1) / float(warmup_epochs)\n",
    "    t = (epoch - warmup_epochs) / float(total_epochs - warmup_epochs)\n",
    "    return 0.0 + 0.5 * (1 - 0.0) * (1 + math.cos(math.pi * t))\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.6, weight_decay=0.0)\n",
    "lr_scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n",
    "\n",
    "# Feel free to adapt and add more arguments\n",
    "# lr = 1e-3\n",
    "# weight_decay = 5e-2\n",
    "# lr_step_size = 10\n",
    "# lr_gamma = 0.1\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "# lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=lr_step_size, gamma=lr_gamma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Training Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adapt your training configuration and implement the training loop. \\\n",
    "You probably want to save model checkpoints and evaluate the model on the validation set at regular intervals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = total_epochs\n",
    "eval_interval = 3  # Evaluate the model every 'eval_interval' epochs\n",
    "save_interval = 10  # Save the model every 'save_interval' epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with wandb.init(project=\"cs461_assignment1\") as run:\n",
    "\n",
    "    all_train_stats = []\n",
    "    all_val_stats = []\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        # TODO: Implement the training and evaluation loop\n",
    "        model.train()\n",
    "        train_stats = training_step(model, train_loaders, optimizer, custom_loss_function)\n",
    "        all_train_stats.append(train_stats)\n",
    "\n",
    "        log_dict = {**train_stats, 'epoch': epoch + 1}\n",
    "\n",
    "        lr_scheduler.step()\n",
    "\n",
    "        if (epoch + 1) % eval_interval == 0:\n",
    "            model.eval()\n",
    "            val_stats = evaluation_step(model, train_loader_no_augment, val_loader_no_augment, oods_train_loader, oods_val_loader)\n",
    "            all_val_stats.append(val_stats)\n",
    "            log_dict.update(val_stats)\n",
    "            print(f\"Epoch {epoch+1}/{n_epochs}, Train Loss: {train_stats['loss']:.4f}, Val kNN-5 Accuracy: {val_stats['knn_accuracy']:.2f}, Val Linear Accuracy: {val_stats['linear_accuracy']:.2f}, OOD kNN-5 Accuracy: {val_stats['oods_knn_accuracy']:.2f}, OOD Linear Accuracy: {val_stats['oods_linear_accuracy']:.2f}\")\n",
    "\n",
    "        if (epoch + 1) % save_interval == 0:\n",
    "            checkpoint_path = checkpoints_dir / f'model_epoch_{epoch+1}.safetensors'\n",
    "            save_model(model, checkpoint_path)\n",
    "            print(f\"Model checkpoint saved at {checkpoint_path}\")\n",
    "\n",
    "        wandb.log(log_dict)\n",
    "\n",
    "\n",
    "    # Save the final model\n",
    "    save_model(model, final_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Visualize Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To better understand the performance of your trained model, visualize some results. \\\n",
    "You can visualize:\n",
    "- Sample images from the validation set along with their predicted labels.\n",
    "- Training and validation loss curves over epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 5, figsize=(12, 5))\n",
    "axes[0].plot(range(1, len(all_train_stats)+1), [s[\"loss\"] for s in all_train_stats])\n",
    "axes[0].set_xlabel('Epoch (1 unit corresponds to 1 epoch)')\n",
    "axes[0].set_ylabel('Average NT-Xent Loss')\n",
    "\n",
    "axes[1].plot(range(1, len(all_val_stats)+1), [s[\"knn_accuracy\"] for s in all_val_stats])\n",
    "axes[1].set_xlabel(f'Epoch (1 unit corresponds to {eval_interval} epochs)')\n",
    "axes[1].set_ylabel('k-NN Accuracy')\n",
    "axes[1].set_ylim(0, 1)\n",
    "\n",
    "axes[2].plot(range(1, len(all_val_stats)+1), [s[\"linear_accuracy\"] for s in all_val_stats])\n",
    "axes[2].set_xlabel(f'Epoch (1 unit corresponds to {eval_interval} epochs)')\n",
    "axes[2].set_ylabel('Linear Accuracy')\n",
    "axes[2].set_ylim(0, 1)\n",
    "\n",
    "axes[3].plot(range(1, len(all_val_stats)+1), [s[\"oods_knn_accuracy\"] for s in all_val_stats])\n",
    "axes[3].set_xlabel(f'Epoch (1 unit corresponds to {eval_interval} epochs)')\n",
    "axes[3].set_ylabel('OOD k-NN Accuracy')\n",
    "axes[3].set_ylim(0, 1)\n",
    "\n",
    "axes[4].plot(range(1, len(all_val_stats)+1), [s[\"oods_linear_accuracy\"] for s in all_val_stats])\n",
    "axes[4].set_xlabel(f'Epoch (1 unit corresponds to {eval_interval} epochs)')\n",
    "axes[4].set_ylabel('OOD Linear Accuracy')\n",
    "axes[4].set_ylim(0, 1)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Submission Instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You must submit the following files:\n",
    "- `models.py`: Contains the implementation of your model architecture.\n",
    "- `final_model.safetensors`: The trained model weights saved in the safetensors format.\n",
    "- `report.md`: A brief report summarizing your approach, design choices, and results.\n",
    "- `CS461_Assignment1.ipynb`: The Jupyter notebook containing your code and explanations. Make sure to save your progress before running the cell below.\n",
    "\n",
    "You will submit your assignment under a single folder named `/home/cs461_assignment1_submission` containing the above files. \\\n",
    "Make sure to replace `<SCIPER>`, `<LAST_NAME>`, and `<FIRST_NAME>` with your actual SCIPER number, last name, and first name respectively. \\\n",
    "The following cell will help you move the files into the submission folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "work_dir = Path('.')\n",
    "output_dir = Path.home() / 'cs461_assignment1_submission'\n",
    "\n",
    "if not output_dir.exists():\n",
    "    output_dir.mkdir(parents=True, exist_ok=False)\n",
    "    \n",
    "shutil.copy(final_model_path, output_dir / 'final_model.safetensors')\n",
    "shutil.copy(work_dir / 'models.py', output_dir / 'models.py')\n",
    "shutil.copy(work_dir / 'CS461_Assignment1.ipynb', output_dir / 'CS461_Assignment1.ipynb')\n",
    "shutil.copy(work_dir / 'report.md', output_dir / 'report.md')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that all required files are present in the submission folder before running the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert SCIPER is not None and LAST_NAME is not None and FIRST_NAME is not None, \"Please set your SCIPER, LAST_NAME, and FIRST_NAME variables.\"\n",
    "\n",
    "list_of_files = ['final_model.safetensors', 'models.py', 'CS461_Assignment1.ipynb', 'report.md']\n",
    "files_found = all((output_dir / f).exists() for f in list_of_files)\n",
    "assert files_found, f\"One or more required files are missing in the submission folder: {list_of_files}\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can test whether your submission folder is appropriately structured by using the `eval.py`:\n",
    "```bash\n",
    "python eval.py\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Uncomment the line below to run the evaluation script and check your model's performance\n",
    "\n",
    "!python eval.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "üéâ **Congratulations!**  \n",
    "You‚Äôve completed Assignment 1. Good luck, and don‚Äôt forget to double-check your submission!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
